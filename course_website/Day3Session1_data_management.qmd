---
title: "Data Management for Reproducible Research"
---

## Data Life Cycle

When working with any type of data, it makes sense to sit down before the project starts to think through the different life stages of the data in your project. This will help counteract some of the problems that can arise when projects grow more organically, and will help consistency within the research group, ease of collaboration, and mostly your future self that will understand what past-self has been up to in the project. 

::: {.callout-note}

More and more funding agencies expect a Data Management Plan at some point of a project application. In there, you need to document that you have thought of, and planned for, the life cycle of your data. 

:::


![[The Research Data Management toolkit for Life Sciences](https://rdmkit.elixir-europe.org/)](figures/data_management/Data_life_cycle.png){.class width=50%}


## FAIR principles

In the past, research data was often generated with one question in mind. Often, they would afterwards land in some drawer and be forgoten about. Nowadays researchers acknowledge that data can also be re-used, or combined with other data, to answer different questions. 

The FAIR principles promote efficient data discovery and reuse by providing guidelines to make digital resources:

![[Wilkinson et al. (2016)](https://www.tpximpact.com/knowledge-hub/insights/fair-data-guide/
)](figures/data_management/fair-data-principles.webp){.class width=50%}

FAIR principles, in turn, rely on **good data management practices** in all phases of research:

- Research documentation
- Data organisation
- Information security
- Ethics and legislation

## Reproducible research

Lucky for us, once we implement good data management practices, we will also increase the reproducibility of our analyses. Extensive documentation will increase faith in the outcome of analyses, and will help people (again, future-you) understand what has been done. 


## What data do we work with?

> Bioinformatics is an interdisciplinary field of science that develops methods and software tools for understanding biological data, especially when the data sets are large and complex.
([Wikipedia](https://en.wikipedia.org/wiki/Bioinformatics))

This data can come from a variety of different biological processes:

![source: Lizel Potgieter](figures/data_management/bioinformatics_dogma.png){.class width=70%}



Early on, sequencing data was not readily available, but due to [decreasing costs](https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost) and [increased computational power](https://en.wikipedia.org/wiki/Moore%27s_law) biological data is now being produced in ever increasing quantities:

![[Growth of the Sequence Read Archive, SRA, from 2012 to 2021](https://academic.oup.com/nar/article/50/D1/D387/6438001)](figures/data_management/SRA_available_data.jpg){.class width=70%}


At the same time, new technologies are being developed, and new tools that might or might not be maintained or benchmarked against existing tools. It's the wild west out there!

![Overview of modern sequencing technologies and where they apply to biological processes](figures/data_management/technologies.png){.class width=70%}

## Ways to work on data

> The solution is to approach bioinformatics as a bioinformatician does: try stuff, and
assess the results. In this way, bioinformatics is just about having the skills to experiment with data using a computer and understanding your results. 
[source: Vince Buffalo](https://womengovtcollegevisakha.ac.in/departments/Bioinformatics%20Data%20Skills%20Reproducible%20and%20Robust%20Research%20with%20Open%20Source%20Tools%20by%20Vince%20Buffalo.pdf)


This is a traditional way to work with bioinformatics data, and can still have its merits. However, in this course we would like to introduce you to a more structured way to make sense of your data. 

Let's have a look at how a beginning PhD student might approach their data: 

- They might analyse their data, and get some results. 
- After talking with their supervisor they might get a few other samples from a collaborator, or need to drop them from the analyses due to quality concerns. 
- They run the analyses again and get a different set of results. 
- There might be a few iterations of this process, and then the reviewers require some additional analyses...

In the "end" we have something like this: 

![Which one of these is the latest version?](figures/data_management/data_chaos.png){.class width=20%}

::: {.callout-tip title="Best practices file organization"}
- There is a folder for the raw data, which **does not get altered**.
- Code is **kept separate** from data.
- Use a **version control** system (at least for code) – e.g. git.
- There should be a **README** in every directory, describing the purpose of the directory and its contents.
- Use **file naming** schemes that makes it easy to find files and understand what they are (for humans and machines) and document them.
- Use **non-proprietary formats** – .csv rather than .xlsx
:::

## Literate programming

Our hypothetical PhD student, even if taking into account the best practice tips from above, is still likely to run the same analyses over and over whenever the input data changes. Sometimes, this might be months, or even years, after the original analysis was performed. 

Luckily for our student, they can save their code snippets (with intuitive file names) and re-use the code from back then. This is often done with R-scripts, but can just as well be applied to bash scripts, python scripts etc.

In the past years, the development went even further and one can even combine code and documentation in the same document. The code is wrapped in so called `chunks`, that are executable from within the document. 

These **notebooks** come in different flavors, for example `jupyter notebooks`and `marimo` for Python applications, `Rmarkdown` for R code. Its successor, `quarto` can be used to integrate a variety of coding languanges. In this course, we will introduce you to `quarto`.

## Hands-on: quarto

With this, we can turn to the [quarto hands-on training](Day3Session2_quarto.qmd) to learn more about `quarto` and its capabilities. 


## Version control

Now that our student has reproducible documents, with reasonable names, that can execute their analyses reliably over and over again, what happens if they modify their analyses? Will they end up again with different result files and their project sink down in chaos?

No, because there is `version control`, the practice of tracking and managing changes to files. 

Version control can be used on the local system, where both the version database and the checked out file - the one that is actively being worked on - are on the local computer. Good, but the local computer can be corrupted and then the data is compromised.

![[Local version control](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)](figures/data_management/Local_version_control_diagram.png){.class width=50%}

Version control can also be centralized, where the version database is on a central server, and the active file can be checked out from several different computers. This is useful when working from different systems, or when working with collaborators. However, when the central servers is compromised the historical version are lost. 

![[Centralized version control](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)](figures/data_management/Centralized_version_control.png){.class width=50%}

At last, version control can be fully distributed, with all versions of the file being on the server and different computers. Each computer checks out the file from its own version database to work on them. The databases are then synchronized between the different computers and the server. One such distributed version control system is `git`. It can handle everything from small to very large projects and is simple to use. `GitHub`is a code hosting platform for version control and collaboration, built on git.

![[Distributed version control](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)](figures/data_management/git_distributed_version_control.png){.class width=50%}

## Hands-on: git and github

We have prepared a hands-on training for [git and GitHub](Day3Session3_github.qmd) for you. 

## Environment managers

Using git, our PhD student can now share their reproducible code with their colloaborators, or between systems. They can rest assured that the different versions of the notebook are tracked and can be checked out when necessary. But what about the bioinformatic tools? 

Different computers can run on different operating systems, or can have different versions of databases installed. This can lead to a conflict between tools, or versions and can impact code usability, or reproducibility. 

Fortunately, smart people have developed environment managers such as `conda`, `bioconda`, or `pixi`. These tools find and install packages, so that the same package versions are being run between different computers. However, the code might still give different results on different operating systems.

## Hands-on: managing environments with pixi

Here is a link to our [introduction to pixi](Day1Session3_pixi.qmd).

## Containers in bioinformatics

But what if our PhD student needs to run their code on different operating systems? 

They can use containers, that contain everything needed to run the application, even the operating system. Containers are being exchanged as container images, which makes them lightweight. Containers do not change over time, so the results will be the same today and in a few years. Everyone gets the same container that works in the same way.

## Hands-on: containers

Here is our tutorial on where to get container images, and how to use - and even build your own - [containers](Day2Session1_containers.qmd).


::: {.callout-note}

test

:::


![text](figures/data_management/name.jpg){.class width=70%}