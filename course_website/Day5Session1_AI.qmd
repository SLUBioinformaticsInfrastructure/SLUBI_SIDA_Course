---
title: "Caution: AI in Bioinformatics"
---

## Introduction

In the past 2 years, AI has become more mainstream. The primary kind of AI people think of, is a Large Language Model (LLM). At this point, you cannot really avoid contact with these models anymore as they have infiltrated every aspect of the internet.

![Searching LLM with Google to be answered with an LLM response](figures/llm1.png)

These algorithms are language models trained on incredibly large datasets. They are trained to recognise and generate natural language. The chatbots we are all familiar with are generative pretrained transformers (GPTs). These can be trained for specific tasks, or guided by prompt generation.

This is not an AI theory course, and we really do not have enough knowledge to explain the underlying theory beyond a rudimentary level. We would like to discuss the impact of AI on you as a user, and advocate for responsible use of AI in your current and future work.

## Machine Learning in Bioinformatics

Machine learning (ML) has been used in bioinformatics for many decades on every level. Before ML, algorithms had to be programmed by hand rather than having the algorithms learn features of a dataset. With ML, features of a dataset can be annotated based on previously annotated datasets. These algorithms were a mix of supervised (learning on annotated data) and unsupervised (learning on unannotated data) learning, depending on the function of the algorithm. 

Supervised algorithms are used for classification and regression analyses. Unsupervised algorithms are used to discover hidden patterns in data without needing a human's input. Unsupervised algorithms are used in clustering, association, and dimensionality reduction

::: {.callout-note collapse="true"}

**Classification**: Output is a discrete variable. Linear classifiers, support vector machines, decision trees, random forests. E.g. annotating a new genome based on genome annotations from existing species.

**Regression**: Focus on understanding dependent and indepedent variables 

**Clustering**: 

**Association**:

**Dimensionality reduction**:

:::

For more info, see [here](https://www.ibm.com/think/topics/supervised-vs-unsupervised-learning), [here](https://www.sciencedirect.com/science/article/pii/S0079610719301981), .

There is no arguing that these algorithms have led to great progress within the field of bioinformatics

## Impact of AI

ChatGPT gained 100 million users in the 2 months after its release in 2022, making it the fastest-growing consumr application in [history](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/). Generative AI (GenAI) models now come in many different flavours, depending on the developer. 

![GenAI chatbots by market share in August 2025 from [FirstPageSage](https://firstpagesage.com/reports/top-generative-ai-chatbots/)](figures/genai_marketshare.png)

### Shortcomings We Must Be Aware Of

#### Training Data

GenAI has all been trained on existing data. There has always been a bias in whose information gets captured. In a historical context, history was recorded by the party that won the war and destroyed existing knowledge. This also changed as different empires and narratives rose and fell. With digitisation, this information has landed on the internet. In the more modern "Internet Age", everyone with an internet connection can technically post anything they'd like on the internet. There are different biases at play- not everyone has equal access to the internet, some people may not have strong enough opinions to post about something online, some people prefer to read rather than contribute, while others take pleasure in "shit-posting".

The differences in how people use the internet have an effect on how useful the trained models become. In this example, you can clearly see what the effect of shit-posting is.

![GenAI answer to a simple question](figures/ai_kill_yourself.png)

GenAI's are not programmed to say that they do not know something, and will happily hallucinate an answer. If you do not know better, or trust the computers, you may take this answer as true and post it elsewhere. As the use of AI's increases, AI generated content is used to train new AI's.

In a perfect world, GenAI would be trained on perfectly curated data, but even with all of the data that we have on the internet at the moment, we do not have nearly enough data for this. We have to make do with what we have.

## Some Negatives of Using AI

We have all seen the amazing things we can do with GenAI, so I will not go into detail about how *cool* AI is. I will highlight some of the drawbacks of AI, and things we need to keep an eye on as the use of AI increases. I don't believe we will ever be able to get away from GenAI, but we can make decisions to use AI responsibly.

::: {.callout-caution}

While the lure of AI is becoming more present in our daily lives, remember that you do not **need** it. You were perfectly able to design a packing list for your upcoming trip. You were able to look at a paper to find answers to your scientific questions. You knew how to query a vignette in R to determine how a function could be implemented.

Life was a bit slower, but you used your mind and your agency. You made decisions. Please do not confuse convenience with need!

:::

### Learning with AI

AI has great potential in the field of education. ChatGPT has been shown to be highly beneficial in an educational [environment](https://www.nature.com/articles/s41599-025-04787-y) when integrated properly. However, the use of AI in this setting must be balanced and carefully curated. A 2025 [pre-print](https://arxiv.org/pdf/2506.08872v1) by Kosmyna et al showed that adults that used ChatGPT to write SAT type essays were outperformed consistently by adults that wrote the same essay without the support of an AI, and had significantly lower brain engagement.

### Productivity with AI

A recent [study](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) by a non-profit group, Model Evaluation and Threat Research [(METR)https://metr.org/] aimed to quantify the difference in productivity when using [AI](https://www.theregister.com/2025/07/11/ai_code_tools_slow_down/). Participants in this study were not new to their field, with at least 5 years of experience prior to this study being conducted.

![AI reducing productivity](figures/metr_ai.png)

The study also found that when AI is allowed, the participants spent less time coding and seeking solutions to the problems. Rather, they spent time prompting the AI, reading and reviewing responses, and being idle. Intel produced similar [findings](https://www.theregister.com/2024/11/22/ai_pcs_productivity/).

![Reasons for loss of productivity with AI](figures/metr_reasons.png)

If one combines the effect of passively learning with AI and the loss of productivity with, it becomes clear that AI must be used very carefully

### Data Privacy

### Global Linguistic Changes 

Even though ChatGPT has only been widely used for 3 years, it has already started leaving its traces in how we speak. Words like `delve` and `meticulous` are being used more frequently in academic YouTube talks.

![GPT words in YouTube videos from [Yakura *et al* 2025](https://arxiv.org/pdf/2409.01754)](figures/chatgpt_delve.png)

It has also been shown that different GPT's have different writing styles, also known as idiolects. 

Some projects like [this one](https://aclanthology.org/2024.inlg-main.34.pdf) are trying to match GPT ideolects match that of unique users. This will make detecting AI use more difficult in future.

::: {.callout collapse="true"}

This course was written by two people, and each person wrote their own sections without the use of any AI. Can you tell who wrote which sections based on idiolects?

:::

Can we risk losing our individuality to subtle linguistic shifts? 

We know that subtle linguistic shifts can change [emotional regulation](https://journals.sagepub.com/doi/10.1177/0963721419861411) within an individual. We also know that [sociolinguistics](https://en.wikipedia.org/wiki/Sociolinguistics), even the slightest linguistic features, serve to bind or divide us. At the risk of sounding like an alarmist, we must be vigilant. 

### AI on the Internet

Social media platforms are an important aspect in the development, improvement, and implementation of GenAI models. OpenAI, the creators of ChatGPT, have used a subreddit on the social media platform, Reddit, to train their [new algorithm](https://techcrunch.com/2025/01/31/openai-used-this-subreddit-to-test-ai-persuasion/). [Google](https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/) and [OpenAI](https://techcrunch.com/2024/05/16/openai-inks-deal-to-train-ai-on-reddit-data/) have contractual agreements with Reddit to license data from the users on the platform. Earlier in 2025, researchers from the University of Zurich were implicated in an experiment on the same subreddit OpenAI used to train a model. They wanted to test whether an interaction with a bot was more likely to make people change their minds than an interaction with a [real person](https://www.science.org/content/article/unethical-ai-research-reddit-under-fire). This was heavily frowned upon, and posts were all removed as users had no ability to consent to participating in a study. It has also been suggested that interactions observed by the researchers were just bots arguing with each other.

A preprint released in February 2025 by [Liang *et al*](https://arxiv.org/pdf/2502.09747) found that the amount of content generated by AI rose from 2-3% in November 2022 to 24% by the end of 2023.  

![AI slop trough by [Yahoo! News!](https://uk.news.yahoo.com/entire-internet-being-polluted-ai-140705958.html)](figures/ai_slop.png)

### Environmnetal Impact

The facilities to run GenAI require a significant amount of [resources](https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117). These facilities need a huge amount of electricity to power the facility (this places extreme strain on exising infrastructure and increases the grid's carbon footprint) as well as water to cool the hardware. Currently, data centers use more electricity than many independent countries.

Some data centers are being built near poor communities, drain resources from, and add pollution to the community (see [Colossus](https://www.grokmountain.com/p/origin-of-grok-3-the-colossus-data) that has been built in [Memphis](https://tennesseelookout.com/2025/07/07/a-billionaire-an-ai-supercomputer-toxic-emissions-and-a-memphis-community-that-did-nothing-wrong/) to power the X bot, [Grok](https://www.theguardian.com/technology/2025/apr/24/elon-musk-xai-memphis) for an example. Musk is not the only offender.)

While we cannot do anything about where data centers are built, we can make informed decisions about which platforms (if any), we use. We can also be careful with the number of queries we send, and how we use our queries. In April 2025, the CEO of OpenAI [said](https://www.vice.com/en/article/telling-chatgpt-please-and-thank-you-costs-openai-millions-ceo-claims/) that polite requests like "please" and "thank you" have cost tens of millions of dollars due to the cost of electricity.

## How To Make The Best Of The Situation

If we know the risks and the true cost of what we are doing, we can make informed decisions about how we chose to incorporate new technologies into our day-to-day and working lives.

Here are some questions I ask myself before I even open my GPT of choice

1. Am I phrasing my prompt in a good way? [Here](https://www.pointloma.edu/resources/business-leadership/how-optimize-your-chatgpt-experience-prompt-engineering) is a guide to prompt engineering that might be useful
2. Can I find this information any other way?
3. How much time am I saving by looking this question up here vs on BioStars, for example?
4. Do I know enough about the topic to know whether the GPT is lying to me?
5. What are the consequences of testing the validity of the GPT solution? Can I potentially corrupt my data or my system? Am I lie to someone who trusts me enough to ask my opinion?

If I cannot say with certainty that I will be able to find a hallucination or a lie, I will not put my thoughts to my GPT. If there is even the slightest chance that I am spreading disinformation by repeating information from the GPT, I will not put my thoughts to the GPT.

If I use a GPT to learn a new skill, or show a student how to do this, I highlight how important active learning is. I suggest that they seek explanations for everything themselves, and doublecheck everything the GPT tells them.